{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ChatChain' from 'langchain.chains' (c:\\projects\\video-summarization\\.venv\\lib\\site-packages\\langchain\\chains\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdocstore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdocument\u001b[39;00m \u001b[39mimport\u001b[39;00m Document\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mprompts\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchat\u001b[39;00m \u001b[39mimport\u001b[39;00m PromptTemplate, ChatPromptTemplate\n\u001b[1;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mlangchain\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mchains\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     21\u001b[0m     StuffDocumentsChain,\n\u001b[0;32m     22\u001b[0m     LLMChain,\n\u001b[0;32m     23\u001b[0m     ChatChain,\n\u001b[0;32m     24\u001b[0m     ReduceDocumentsChain,\n\u001b[0;32m     25\u001b[0m     MapReduceDocumentsChain,\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     27\u001b[0m llm \u001b[39m=\u001b[39m AzureOpenAI(\n\u001b[0;32m     28\u001b[0m     deployment_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-35-turbo-16k\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     30\u001b[0m chat_model \u001b[39m=\u001b[39m AzureChatOpenAI(\n\u001b[0;32m     31\u001b[0m     deployment_name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpt-35-turbo-16k\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     32\u001b[0m )\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ChatChain' from 'langchain.chains' (c:\\projects\\video-summarization\\.venv\\lib\\site-packages\\langchain\\chains\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "source_dir = \"C:\\\\projects\\\\local\\\\video_analysis\"\n",
    "ffmpeg_path = os.environ['LOCALAPPDATA'] + \"\\\\Microsoft\\\\WinGet\\\\Packages\\\\Gyan.FFmpeg_Microsoft.Winget.Source_8wekyb3d8bbwe\\\\ffmpeg-6.0-full_build\\\\bin\\\\ffmpeg.exe\"\n",
    "import whisper\n",
    "os.environ[\"OPENAI_API_TYPE\"]=\"azure\"\n",
    "os.environ[\"OPENAI_API_VERSION\"]=\"2023-05-15\"\n",
    "\n",
    "# get settings from files\n",
    "with open(source_dir + \"\\\\..\\\\openai-base.txt\", \"r\") as f:\n",
    "    os.environ[\"OPENAI_API_BASE\"] = f.read()\n",
    "with open(source_dir + \"\\\\..\\\\openai-key.txt\", \"r\") as f:\n",
    "    os.environ[\"OPENAI_API_KEY\"] = f.read()\n",
    "\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.prompts.chat import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.chains import (\n",
    "    StuffDocumentsChain,\n",
    "    LLMChain,\n",
    "    ChatChain,\n",
    "    ReduceDocumentsChain,\n",
    "    MapReduceDocumentsChain,\n",
    ")\n",
    "llm = AzureOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo-16k\"\n",
    ")\n",
    "chat_model = AzureChatOpenAI(\n",
    "    deployment_name=\"gpt-35-turbo-16k\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all mp4 files in source_dir (and subdirectories) that do not yet have a mp3 file and convert them with ffmpeg\n",
    "def process_files():\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".mp4\"):\n",
    "                mp4_file = os.path.join(root, file)\n",
    "                mp3_file = mp4_file[:-4] + '.mp3'\n",
    "                if not os.path.isfile(mp3_file):\n",
    "                    cmd = [ffmpeg_path, '-i', mp4_file, '-vn', '-ar', '44100', '-ac', '2', '-ab', '192k', '-f', 'mp3', mp3_file]\n",
    "                    print(cmd)\n",
    "                    proc = subprocess.Popen(cmd)\n",
    "                    result = proc.wait()\n",
    "                    print(\"{} - processed from {}\".format(result, mp4_file))\n",
    "process_files()\n",
    "print('generated MP3s for all MP4s in ' + source_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisper.load_model('medium.en', device='cuda')\n",
    "\n",
    "# find all mp3 files in source_dir (and subdirectories) that do not yet have a txt file and convert them with whisper\n",
    "def process_files():\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".mp3\"):\n",
    "                mp3_file = os.path.join(root, file)\n",
    "                txt_file = mp3_file[:-4] + '.txt'\n",
    "                if not os.path.isfile(txt_file):\n",
    "                    print('processing {}'.format(mp3_file))\n",
    "                    result = model.transcribe(mp3_file)\n",
    "                    # result has the following structure:\n",
    "                    #  text: string\n",
    "                    #  language: string\n",
    "                    #  segments: array of\n",
    "                    #    id: number\n",
    "                    #    seek: number\n",
    "                    #    start: number\n",
    "                    #    end: number\n",
    "                    #    text: string\n",
    "                    #    tokens: number[]\n",
    "                    #    temperature: number\n",
    "                    #    avg_logprob: number\n",
    "                    #    compression_ratio: number\n",
    "                    #    no_speech_prob: number\n",
    "\n",
    "                    # if we used text, we'll get one giant line.  Instead, we'll use segments\n",
    "                    print('got {} segments from {}'.format(len(result['segments']), mp3_file))\n",
    "                    with open(txt_file, 'w') as f:\n",
    "                        for segment in result['segments']:\n",
    "                            f.write(segment['text'] + '\\n')\n",
    "                    print('wrote to {}'.format(txt_file))\n",
    "process_files()\n",
    "print('generated TXTs for all MP3s in ' + source_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a helpful assistant that identifies the date a file was created based on information in the filename.\n",
    "    All dates are between 2019 and 2030, and any dates in the filename are written with the month before the date (ie. american style).\n",
    "    The resulting date should be formatted as YYYY-mm-dd - ie. 2021-02-15.\n",
    "    Your response should _only_ contain the date, and nothing else.\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{text}\"),\n",
    "])\n",
    "chain = chat_prompt | chat_model\n",
    "\n",
    "# find all txt files in source_dir (and subdirectories) that do not yet have a date file and ask the LLM to guess the date\n",
    "def process_files():\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".txt\"):\n",
    "                txt_file = os.path.join(root, file)\n",
    "                date_file = txt_file[:-4] + '.date'\n",
    "                if not os.path.isfile(date_file):\n",
    "                    print('getting date for {}'.format(txt_file))\n",
    "                    result = chain.invoke({ \"text\": txt_file})\n",
    "                    date = result.content\n",
    "                    # use a regular expression to make sure date looks like YYYY-mm-dd\n",
    "                    if not re.match(r'^\\d{4}-\\d{2}-\\d{2}$', date):\n",
    "                        print('invalid date: {} generated for {}'.format(date, txt_file))\n",
    "                        continue\n",
    "                    with open(date_file, 'w') as f:\n",
    "                        f.write(date)\n",
    "process_files()\n",
    "print('generated DATEs for all TXTs in ' + source_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting answer for 2021-10-28: C:\\projects\\local\\video_analysis\\2021\\All Company Meeting_10.28.21\\2021-10-28 RP All Company Meeting.txt\n",
      "writing answer to C:\\projects\\local\\video_analysis\\2021\\All Company Meeting_10.28.21\\2021-10-28 RP All Company Meeting.marketing\n",
      "During the meeting, the content discussed various topics including Breast Cancer Awareness Month, updates on the company's one-year anniversary, digital health and therapeutics offerings, and end-of-year reminders. However, it failed to directly address the questions regarding the marketing team's activities or the results of marketing efforts.\n",
      "generated marketings for all TXTs in C:\\projects\\local\\video_analysis\n"
     ]
    }
   ],
   "source": [
    "# ok, now for the real work.\n",
    "#  1. given a question and answer-extension, find all files with a txt+date file, but no answer-extension file\n",
    "#  2. ask the LLM to answer the question based on the file\n",
    "#  3. write the answer to the answer-extension file\n",
    "\n",
    "with open(source_dir + \"\\\\question.input\", \"r\") as f:\n",
    "    question = f.read().strip()\n",
    "with open(source_dir + \"\\\\answer-extension.input\", \"r\") as f:\n",
    "    answerExt = f.read().strip()\n",
    "\n",
    "# heavily based on https://github.com/langchain-ai/langchain/blob/master/libs/langchain/langchain/chains/combine_documents/map_reduce.py\n",
    "document_prompt = PromptTemplate(\n",
    "    input_variables=[\"page_content\"],\n",
    "    template=\"{page_content}\"\n",
    ")\n",
    "document_variable_name = \"context\"\n",
    "# The prompt here should take as an input variable the\n",
    "# `document_variable_name`\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are a helpful assistant that summarizes existing content with the goal of answering these questions:\n",
    "    {}\n",
    "    If the content supplied isn't relevant to the question, you will say so.\n",
    "    \"\"\".format(question) + \"Summarize this content: {context}\"\n",
    ")\n",
    "llm_chain = LLMChain(llm=chat_model, prompt=prompt)\n",
    "# chat_chain = chat_prompt | chat_model\n",
    "# chat_chain = ChatChain()\n",
    "# We now define how to combine these summaries\n",
    "reduce_prompt = PromptTemplate.from_template(\n",
    "    \"Combine these summaries: {context}\"\n",
    ")\n",
    "reduce_llm_chain = LLMChain(llm=chat_model, prompt=reduce_prompt)\n",
    "combine_documents_chain = StuffDocumentsChain(\n",
    "    llm_chain=reduce_llm_chain,\n",
    "    document_prompt=document_prompt,\n",
    "    document_variable_name=document_variable_name\n",
    ")\n",
    "reduce_documents_chain = ReduceDocumentsChain(\n",
    "    combine_documents_chain=combine_documents_chain,\n",
    ")\n",
    "chain = MapReduceDocumentsChain(\n",
    "    llm_chain=llm_chain,\n",
    "    reduce_documents_chain=reduce_documents_chain,\n",
    ")\n",
    "\n",
    "# find all txt files in source_dir (and subdirectories) that do not yet have a date file and ask the LLM to guess the date\n",
    "def process_files():\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(\".date\"):\n",
    "                date_file = os.path.join(root, file)\n",
    "                txt_file = date_file[:-5] + '.txt'\n",
    "                if os.path.isfile(txt_file):\n",
    "                    answer_file = date_file[:-5] + '.' + answerExt\n",
    "                    if not os.path.isfile(answer_file):\n",
    "                        with open(txt_file, \"r\") as f:\n",
    "                            content = f.read().strip()\n",
    "                        with open(date_file, \"r\") as f:\n",
    "                            date = f.read().strip()\n",
    "                        print('getting answer for {}: {}'.format(date, txt_file))\n",
    "                        result = chain.invoke({ \"input_documents\": [Document(page_content=content)] })\n",
    "                        answer = result['output_text']\n",
    "                        print('writing answer to {}'.format(answer_file))\n",
    "                        with open(answer_file, 'w') as f:\n",
    "                            f.write(answer)\n",
    "                        return\n",
    "process_files()\n",
    "print('generated {}s for all TXTs in {}'.format(answerExt, source_dir))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
